---
title: "Artificial neural networks"
teaching: 15
exercises: 0
questions:
- "What do we need a template for?"
objectives:
- "Edit lesson materials in the template"
- "Contribute fixes through Github PRs"
- "Use the template to create your own lesson"
keypoints:
- We want to use this template to provide lesson materials in an open and useful format.
- This is in line with our overall goal of making science (including scientific training) more open.

---

### Why neural networks? Why now?

Artifical neural networks are not a new idea at all. Arguably, these
ideas were around at the dawn of computer science, in the 1940's, and
were clearly articulated in work that continued in the 1950's and 1960's.
In the 1980's this thinking was put to practice in the work of several
groups including the PDP (parallel distributed programming) research group.
But it was really only in the 1990's that computers could be programmed to do
highly useful things with neural networks. For example, the seminal work of
Yann LeCun developing convolutional neural networks that were able to read
hand-written digits.

However, the implementation and application of neural networks to many
different problems really took off in the early 2010's with work that
spanned academic and industry research labs (e.g., work from Andrew Ng's
group at Stanford, in collaboration with researchers at Google, as well
as work from Geoff Hinton's group and Yoshua Bengio, both at the time in
Canada, Toronto and Montreal, respectively).

Some of the factors that ushered in the current golden age of artificial
neural networks are:

1. Development of hardware and software to implement large neural
   networks
2. The availability of large datasets for training these algorithms
3. Theoretical developments across a wide range of fields, including
   ideas closely related to the structure of these networks, but also in
   cognate fields, such as optimization.

One of the landmark datasets that allow us to track these developments results for the field was the